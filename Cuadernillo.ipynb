{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyConES 2022: Chatbots, Reconocimiento de Voz y Text-to-Speech: Tu Asistente Virtual Open-Source \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manejando el audio desde nuestro PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manejar la grabación y la reproducción de audio en nuestro ordenador a través de Python podemos usar PyAudio.\n",
    "\n",
    "| Nombre      | PyAudio                                                                                                                                                                                                                                                                             |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Licencia    | MIT                                                                                                                                                                                                                                                                                 |\n",
    "| Descripción | (Traducido) PyAudio proporciona enlaces de Python para PortAudio v19, la biblioteca de E/S de audio multiplataforma. Con PyAudio, puedes utilizar fácilmente Python para reproducir y grabar audio en una variedad de plataformas, como GNU/Linux, Microsoft Windows y Apple macOS. |\n",
    "\n",
    "Los archivos a grabar/reproducir usan el estándar WAVE para poder extraer toda la información.\n",
    "\n",
    "### 1.1 Muestreo y precisión\n",
    "En el mundo del audio digital, todo funciona por 1s y 0s. Pero el sonido es realmente analógico, por lo que hay que parametrizar el mundo analógico en datos discretos. De esta manera, necesitamos recoger con cierta frecuencia la información, y también alguna manera de representar la intensidad de la información del audio.\n",
    "\n",
    "A la frecuencia de recogida se le conoce como **muestreo**.\n",
    "\n",
    "A el nivel de detalle que podemos recoger la información del audio en digital se le conoce como **precisión** o **profundidad**. Cuantos más bits se requieran para tomar el dato, más profundidad tendrá.\n",
    "\n",
    "\n",
    "![Muestreo](img/muestreo.png)\n",
    "![Precisión](img/precision.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha acabado la reproducción\n"
     ]
    }
   ],
   "source": [
    "# Script de reproducción\n",
    "import wave\n",
    "import pyaudio\n",
    "\n",
    "pyaudio_play = pyaudio.PyAudio()\n",
    "archivo = wave.open('file.wav','rb')\n",
    "\n",
    "stream = pyaudio_play.open(\n",
    "    format = pyaudio_play.get_format_from_width(archivo.getsampwidth()),\n",
    "    channels = archivo.getnchannels(),\n",
    "    rate = archivo.getframerate(),\n",
    "    output = True\n",
    ")\n",
    "\n",
    "datos = archivo.readframes(1024)\n",
    "\n",
    "while len(datos) > 0:\n",
    "        # writing to the stream is what *actually* plays the sound.\n",
    "    stream.write(datos)\n",
    "    datos = archivo.readframes(1024)\n",
    "\n",
    "pyaudio_play.close(stream)\n",
    "print(\"Ha acabado la reproducción\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording...\n",
      "1\n",
      "1\n",
      "1\n",
      "262\n",
      "265\n",
      "307\n",
      "270\n",
      "318\n",
      "505\n",
      "294\n",
      "305\n",
      "303\n",
      "309\n",
      "340\n",
      "331\n",
      "322\n",
      "309\n",
      "339\n",
      "350\n",
      "421\n",
      "32516\n",
      "32555\n",
      "31479\n",
      "29391\n",
      "12730\n",
      "5306\n",
      "291\n",
      "Silencio 1\n",
      "220\n",
      "Silencio 2\n",
      "266\n",
      "Silencio 3\n",
      "32362\n",
      "32546\n",
      "22565\n",
      "27203\n",
      "32563\n",
      "1870\n",
      "Silencio 1\n",
      "683\n",
      "Silencio 2\n",
      "24159\n",
      "32677\n",
      "31870\n",
      "21462\n",
      "5010\n",
      "182\n",
      "Silencio 1\n",
      "169\n",
      "Silencio 2\n",
      "321\n",
      "Silencio 3\n",
      "271\n",
      "Silencio 4\n",
      "250\n",
      "Silencio 5\n",
      "360\n",
      "Silencio 6\n",
      "237\n",
      "Silencio 7\n",
      "229\n",
      "Silencio 8\n",
      "194\n",
      "Silencio 9\n",
      "179\n",
      "Silencio 10\n",
      "finished recording\n"
     ]
    }
   ],
   "source": [
    "# Script de grabación\n",
    "from array import array\n",
    "import pyaudio\n",
    "import wave\n",
    " \n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
    "THRESHOLD = 2000\n",
    " \n",
    "audio = pyaudio.PyAudio()\n",
    "audio_data = []\n",
    "can_record = False\n",
    "finish_record = False\n",
    "silent_chunks = 0\n",
    " \n",
    "# start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                rate=RATE, input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "print (\"recording...\")\n",
    " \n",
    "\n",
    "while not finish_record:\n",
    "    chunk = stream.read(CHUNK)\n",
    "    data_array = array('h', chunk)\n",
    "    print(max(data_array))\n",
    "    if not can_record:\n",
    "        can_record = max(data_array) >= THRESHOLD\n",
    "    else:\n",
    "        audio_data.append(chunk)\n",
    "        if (max(data_array) <= THRESHOLD):\n",
    "            silent_chunks += 1\n",
    "            print(\"Silencio {0}\".format(silent_chunks))\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks >= 10:\n",
    "            finish_record = True\n",
    "    \n",
    "\n",
    "\n",
    "print (\"finished recording\")\n",
    " \n",
    " \n",
    "# stop Recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    " \n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(audio_data))\n",
    "waveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reconocimiento de voz. Convirtiendo el sonido en texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Reconocimiento de Voz o Automatic Speech Recognition (ASR) como la capacidad de un programa de procesar el habla y convertirlo en un texto escrito legible, que puedan entender también las máquinas para el posterior procesado de la información.\n",
    "\n",
    "Para reconocer la voz del contenedor del audio, este se extrae en trocitos o chunks que se tratan posteriormente para sacar un texto plano.\n",
    "\n",
    "En nuestro caso usaremos **Vosk** ya que es muy efectivo y los modelos son muy ligeros.\n",
    "\n",
    "| Nombre      | Vosk                                                                                                                                                                                                                                                                             |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Licencia    | Apache License 2.0                                                                                                                                                                                                                                                                                 |\n",
    "| Descripción | Vosk es un conjunto de herramientas de reconocimiento de voz con una API de streaming para la mejor experiencia de usuario. También hay bindings para diferentes lenguajes de programación: java/csharp/javascript, etc. Permite una rápida reconfiguración del vocabulario para obtener la mejor precisión.|\n",
    "\n",
    "Los modelos de Vosk se pueden extraer de [este link](https://alphacephei.com/vosk/models) y para probarlo solo hay que extraer el ZIP y llamar en el código a la carpeta del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total\n",
      "hola buenos días\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Script de reconocimiento.\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import sys\n",
    "import json\n",
    "\n",
    "model = Model(model_path='model')\n",
    "\n",
    "# Large vocabulary free form recognition\n",
    "rec = KaldiRecognizer(model, 16000)\n",
    "\n",
    "wf = open('file.wav', \"rb\")\n",
    "wf.read(44) # skip header\n",
    "\n",
    "while True:\n",
    "    data = wf.read(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        res = json.loads(rec.Result())\n",
    "        print('Partial')\n",
    "        print (res['text'])\n",
    "\n",
    "res = json.loads(rec.FinalResult())\n",
    "print('Total')\n",
    "print (res['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text-to-Speech. Convirtiendo el texto en voz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el reconocimiento de voz trata el procesamiento del habla para extraer un texto, la síntesis de voz sería lo contrario, la producción artificial de ese habla.\n",
    "Para ello se usan como base los grafemas (aquellas letras y grupos que se pronuncian de una manera). De esa forma, se separa el texto en dichos grafemas, tras lo cual se asocian esos grafemas a sus correspondientes sonidos o fonemas asociados, entonando así cada palabra, frase y finalmente leyendo el texto.\n",
    "\n",
    "En este caso usaremos [eSpeakNG](https://github.com/espeak-ng/espeak-ng), por su facilidad de uso. Se puede instalar fácilmente en Linux y Windows (y en MacOS se está trabajando en un port). Por contra, la voz no es muy natural.\n",
    "\n",
    "| Nombre      | eSpeakNG                                                                                                                                                                                                                                                                             |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Licencia    | Apache License 2.0                                                                                                                                                                                                                                                                                 |\n",
    "| Descripción | |\n",
    "\n",
    "¡Ojo! Para usarse en Python sigue los pasos que indican en el paquete de pip (https://pypi.org/project/espeakng/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script de síntesis de voz\n",
    "from espeakng import Speaker\n",
    "\n",
    "esng = Speaker()\n",
    "\n",
    "esng.wpm = 140\n",
    "esng.pitch = 120\n",
    "esng.voice = 'es'\n",
    "esng.say(\"Hola mundo, ¿qué tal estáis?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chatbots y cómo nos responde una IA.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Creando modelos en Rasa\n",
    "\n",
    "Para crear un modelo en Rasa invocaremos el siguiente comando.\n",
    "\n",
    "```bash\n",
    "rasa init\n",
    "```\n",
    "\n",
    "### 4.2 Entrenamiento y pruebas\n",
    "\n",
    "Para entrenar el modelo en Rasa invocaremos el siguiente comando.\n",
    "```bash\n",
    "rasa train\n",
    "```\n",
    "\n",
    "Para probarlo, invocaremos este otro comando.\n",
    "```bash\n",
    "rasa shell\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conectando el servidor de Rasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner a Rasa en modo de servidor, podremos lanzar este comando:\n",
    "\n",
    "```bash\n",
    "rasa run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "sender = input(\"What is your name?\\n\")\n",
    "\n",
    "bot_message = \"\"\n",
    "while bot_message != \"Adiós\":\n",
    "    message = input(\"What's your message?\\n\")\n",
    "\n",
    "    print(\"Sending message now...\")\n",
    "\n",
    "    r = requests.post('http://localhost:5005/webhooks/rest/webhook', json={\"sender\": sender, \"message\": message})\n",
    "\n",
    "    print(r.json())\n",
    "\n",
    "    print(\"Bot says, \")\n",
    "    for i in r.json():\n",
    "        try:\n",
    "            bot_message = i['text']\n",
    "        except KeyError:\n",
    "            try:\n",
    "                bot_message = i['image']\n",
    "            except KeyError:\n",
    "                bot_message = ''\n",
    "        print(f\"{bot_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Funciones custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones custom se pueden lanzar usando:\n",
    "\n",
    "```bash\n",
    "rasa run actions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el archivo chatbot/actions/actions.py ...\n",
    "\n",
    "from rasa_sdk import Action, Tracker\n",
    "from rasa_sdk.executor import CollectingDispatcher\n",
    "import requests\n",
    "\n",
    "class ActionTellTime(Action):\n",
    "    \"\"\"\n",
    "    Clase para la acción de decir la hora\n",
    "    \"\"\"\n",
    "\n",
    "    def name(self) -> Text:\n",
    "        \"\"\"\n",
    "        Declaración de la acción\n",
    "        @return string Nombre de la acción\n",
    "        \"\"\"\n",
    "        return \"action_tell_time\"\n",
    "\n",
    "    def run(self, dispatcher: CollectingDispatcher,\n",
    "            tracker: Tracker,\n",
    "            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n",
    "        \"\"\"\n",
    "        Ejecución de la acción.\n",
    "        Extrae el datetime de este instante y extrae la hora\n",
    "        @utter_message Devuelve la hora en formato HH y MM (15:30 -> Son las 15 horas y 30 minutos)\n",
    "        \"\"\"\n",
    "        dispatcher.utter_message(text=f\"Son las \\\n",
    "            {datetime.datetime.now().strftime('%H horas y %M minutos')}\")\n",
    "\n",
    "        return []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ba91e5e8cac5424b3e03084f20effd4266e6668cb94b70218f91abde236bd39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
