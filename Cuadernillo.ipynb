{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyConES 2022: Chatbots, Reconocimiento de Voz y Text-to-Speech: Tu Asistente Virtual Open-Source \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ¿Qué vamos a necesitar?\n",
    "\n",
    "Necesitaremos Python 3.7.X y las siguientes herramientas que podremos descargar a través de pip\n",
    "\n",
    "- Rasa\n",
    "- PyAudio (requiere el paquete de PortAudio en Linux/Mac)\n",
    "- eSpeakNG (requiere descargarse el programa) \n",
    "- Vosk\n",
    "\n",
    "> **Note**:\n",
    "> \n",
    "> Para el caso de Mac OS, eSpeakNG se puede instalar usando Homebrew o MacPorts, pero si no os funcionara podéis compilar el código fuente\n",
    "> de esta manera:\n",
    "> ```bash\n",
    "> # PCAudioLib (Biblioteca de soporte para eSpeakNG)\n",
    "> brew install autoconf automake libtool\n",
    "> git clone https://github.com/espeak-ng/pcaudiolib.git\n",
    "> cd pcaudiolib\n",
    "> mv CHANGELOG.md ChangeLog.md\n",
    "> ./autogen.sh\n",
    "> ./configure --prefix=/usr/local\n",
    "> make\n",
    "> sudo make install\n",
    "> cd ..\n",
    ">\n",
    "># eSpeakNG, ahora sí\n",
    "> git clone https://github.com/espeak-ng/espeak-ng.git\n",
    "> cd espeak-ng\n",
    "> mv CHANGELOG.md ChangeLog.md\n",
    "> ./configure --exec-prefix=/usr/local/ --datarootdir=/usr/ --sysconfdir=/usr/local --sharedstatedir=/usr/local --localstatedir=/usr/local --includedir=/usr/local --with-extdict-ru --with-extdict-zh --with-extdict-zhy\n",
    "> make\n",
    "> sudo make LIBDIR=/usr/local/lib install\n",
    "> ```\n",
    "\n",
    "\n",
    "> **Note**:\n",
    "> \n",
    "> En el caso de Windows, una vez se instale eSpeakNG, hay que meter la ruta del ejecutable\n",
    "> en el PATH, en el apartado \"Variables de entorno del sistema\" (la ruta sería algo como `C:\\Program Files\\eSpeak NG\\`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasa\n",
    "!pip install pyaudio\n",
    "!pip install espeakng\n",
    "!pip install vosk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manejando el audio desde nuestro PC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manejar la grabación y la reproducción de audio en nuestro ordenador a través de Python podemos usar PyAudio.\n",
    "\n",
    "| Nombre      | PyAudio                                                                                                                                                                                                                                                                             |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Licencia    | MIT                                                                                                                                                                                                                                                                                 |\n",
    "| Descripción | (Traducido) PyAudio proporciona enlaces de Python para PortAudio v19, la biblioteca de E/S de audio multiplataforma. Con PyAudio, puedes utilizar fácilmente Python para reproducir y grabar audio en una variedad de plataformas, como GNU/Linux, Microsoft Windows y Apple macOS. |\n",
    "\n",
    "Los archivos a grabar/reproducir usan el estándar WAVE para poder extraer toda la información.\n",
    "\n",
    "### 1.1 Muestreo y precisión\n",
    "En el mundo del audio digital, todo funciona por 1s y 0s. Pero el sonido es realmente analógico, por lo que hay que parametrizar el mundo analógico en datos discretos. De esta manera, necesitamos recoger con cierta frecuencia la información, y también alguna manera de representar la intensidad de la información del audio.\n",
    "\n",
    "A la frecuencia de recogida se le conoce como **muestreo**.\n",
    "\n",
    "A el nivel de detalle que podemos recoger la información del audio en digital se le conoce como **precisión** o **profundidad**. Cuantos más bits se requieran para tomar el dato, más profundidad tendrá.\n",
    "\n",
    "\n",
    "![Muestreo](img/muestreo.png)\n",
    "![Precisión](img/precision.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha acabado la reproducción\n"
     ]
    }
   ],
   "source": [
    "# Script de reproducción\n",
    "import wave\n",
    "import pyaudio\n",
    "\n",
    "pyaudio_play = pyaudio.PyAudio()\n",
    "archivo = wave.open('file.wav','rb')\n",
    "\n",
    "stream = pyaudio_play.open(\n",
    "    format = pyaudio_play.get_format_from_width(archivo.getsampwidth()),\n",
    "    channels = archivo.getnchannels(),\n",
    "    rate = archivo.getframerate(),\n",
    "    output = True\n",
    ")\n",
    "\n",
    "datos = archivo.readframes(2048)\n",
    "\n",
    "while len(datos) > 0:\n",
    "        # writing to the stream is what *actually* plays the sound.\n",
    "    stream.write(datos)\n",
    "    datos = archivo.readframes(2048)\n",
    "\n",
    "pyaudio_play.close(stream)\n",
    "print(\"Ha acabado la reproducción\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording...\n",
      "1\n",
      "1\n",
      "1\n",
      "4728\n",
      "3487\n",
      "3408\n",
      "10997\n",
      "12401\n",
      "9628\n",
      "9754\n",
      "17171\n",
      "19317\n",
      "17485\n",
      "14549\n",
      "32472\n",
      "32021\n",
      "18584\n",
      "14577\n",
      "22271\n",
      "25081\n",
      "18301\n",
      "5836\n",
      "14194\n",
      "14212\n",
      "8302\n",
      "24855\n",
      "32476\n",
      "25407\n",
      "29193\n",
      "27249\n",
      "26621\n",
      "27691\n",
      "20272\n",
      "10742\n",
      "16879\n",
      "15585\n",
      "295\n",
      "Silencio 1\n",
      "549\n",
      "Silencio 2\n",
      "86\n",
      "Silencio 3\n",
      "80\n",
      "Silencio 4\n",
      "52\n",
      "Silencio 5\n",
      "134\n",
      "Silencio 6\n",
      "123\n",
      "Silencio 7\n",
      "107\n",
      "Silencio 8\n",
      "71\n",
      "Silencio 9\n",
      "81\n",
      "Silencio 10\n",
      "finished recording\n"
     ]
    }
   ],
   "source": [
    "# Script de grabación\n",
    "from array import array\n",
    "import pyaudio\n",
    "import wave\n",
    " \n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
    "THRESHOLD = 2000\n",
    " \n",
    "audio = pyaudio.PyAudio()\n",
    "audio_data = []\n",
    "can_record = False\n",
    "finish_record = False\n",
    "silent_chunks = 0\n",
    " \n",
    "# start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                rate=RATE, input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "print (\"recording...\")\n",
    " \n",
    "\n",
    "while not finish_record:\n",
    "    chunk = stream.read(CHUNK)\n",
    "    data_array = array('h', chunk)\n",
    "    print(max(data_array))\n",
    "    if not can_record:\n",
    "        can_record = max(data_array) >= THRESHOLD\n",
    "    else:\n",
    "        audio_data.append(chunk)\n",
    "        if (max(data_array) <= THRESHOLD):\n",
    "            silent_chunks += 1\n",
    "            print(\"Silencio {0}\".format(silent_chunks))\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks >= 10:\n",
    "            finish_record = True\n",
    "    \n",
    "\n",
    "\n",
    "print (\"finished recording\")\n",
    " \n",
    " \n",
    "# stop Recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    " \n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(audio_data))\n",
    "waveFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reconocimiento de voz. Convirtiendo el sonido en texto. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Reconocimiento de Voz o Automatic Speech Recognition (ASR) como la capacidad de un programa de procesar el habla y convertirlo en un texto escrito legible, que puedan entender también las máquinas para el posterior procesado de la información.\n",
    "\n",
    "Para reconocer la voz del contenedor del audio, este se extrae en trocitos o chunks que se tratan posteriormente para sacar un texto plano.\n",
    "\n",
    "En nuestro caso usaremos **Vosk** ya que es muy efectivo y los modelos son muy ligeros.\n",
    "\n",
    "| Nombre      | Vosk                                                                                                                                                                                                                                                                             |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Licencia    | Apache License 2.0                                                                                                                                                                                                                                                                                 |\n",
    "| Descripción | Vosk es un conjunto de herramientas de reconocimiento de voz con una API de streaming para la mejor experiencia de usuario. También hay bindings para diferentes lenguajes de programación: java/csharp/javascript, etc. Permite una rápida reconfiguración del vocabulario para obtener la mejor precisión.|\n",
    "\n",
    "Los modelos de Vosk se pueden extraer de [este link](https://alphacephei.com/vosk/models) y para probarlo solo hay que extraer el ZIP y llamar en el código a la carpeta del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total\n",
      "una una una una hora\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Script de reconocimiento.\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import sys\n",
    "import json\n",
    "\n",
    "model = Model(model_path='model')\n",
    "\n",
    "# Large vocabulary free form recognition\n",
    "rec = KaldiRecognizer(model, 16000)\n",
    "\n",
    "wf = open('file.wav', \"rb\")\n",
    "wf.read(44) # skip header\n",
    "\n",
    "while True:\n",
    "    data = wf.read(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        res = json.loads(rec.Result())\n",
    "        print('Partial')\n",
    "        print (res['text'])\n",
    "\n",
    "res = json.loads(rec.FinalResult())\n",
    "print('Total')\n",
    "print (res['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text-to-Speech. Convirtiendo el texto en voz."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el reconocimiento de voz trata el procesamiento del habla para extraer un texto, la síntesis de voz sería lo contrario, la producción artificial de ese habla.\n",
    "Para ello se usan como base los grafemas (aquellas letras y grupos que se pronuncian de una manera). De esa forma, se separa el texto en dichos grafemas, tras lo cual se asocian esos grafemas a sus correspondientes sonidos o fonemas asociados, entonando así cada palabra, frase y finalmente leyendo el texto.\n",
    "\n",
    "En este caso usaremos [eSpeakNG](https://github.com/espeak-ng/espeak-ng), por su facilidad de uso. Se puede instalar fácilmente en Linux y Windows (y en MacOS se está trabajando en un port). Por contra, la voz no es muy natural.\n",
    "\n",
    "| Nombre      | eSpeakNG                                                                                                                                                                                                                                                                             |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Licencia    | Apache License 2.0                                                                                                                                                                                                                                                                                 |\n",
    "| Descripción | |\n",
    "\n",
    "¡Ojo! Para usarse en Python sigue los pasos que indican en el paquete de pip (https://pypi.org/project/espeakng/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script de síntesis de voz\n",
    "from espeakng import Speaker\n",
    "\n",
    "esng = Speaker()\n",
    "\n",
    "#esng.wpm = 130\n",
    "#esng.pitch = 200\n",
    "esng.voice = 'es'\n",
    "esng.say(\"Bienvenidos a este nuevo tutorial hoy os enseñamos a descargar el GTA 6 yo no lo descargo porque lo tengo xdxdxd.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chatbots y cómo nos responde una IA.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Creando modelos en Rasa\n",
    "\n",
    "Rasa es un framework de aprendizaje automático de código abierto para automatizar conversaciones basadas en texto y voz.\n",
    "\n",
    "Rasa ayuda a crear asistentes contextuales capaces de mantener conversaciones por capas con muchas idas y venidas. Para que un humano tenga un intercambio significativo con un asistente contextual, el asistente debe ser capaz de utilizar el contexto para construir sobre las cosas que se discutieron previamente - Rasa le permite construir asistentes que pueden hacer esto de una manera escalable.\n",
    "\n",
    "Para crear un modelo en Rasa invocaremos el siguiente comando.\n",
    "\n",
    "```bash\n",
    "rasa init\n",
    "```\n",
    "\n",
    "Con ello se crea un árbol parecido a este:\n",
    "\n",
    "```\n",
    "chatbot\n",
    "|-- .rasa\n",
    "|-- cache (Incluye todos los archivos temporales de entrenamiento de Rasa)\n",
    "|-- actions\n",
    "||-- actions.py\n",
    "||-- __init__.py\n",
    "|-- data\n",
    "||-- nlu.yml\n",
    "||-- rules.yml\n",
    "||-- stories.yml\n",
    "|-- models (Incluye los modelos en formato .tar.gz)\n",
    "|-- tests\n",
    "||-- test_stories.yml\n",
    "|-- config.yml\n",
    "|-- credentials.yml\n",
    "|-- domain.yml\n",
    "|-- endpoints.yml\n",
    "```\n",
    "\n",
    "### 4.2 Entrenamiento y pruebas\n",
    "\n",
    "Rasa tiene como unidades básicas para entender la pregunta las intenciones (o intents), que son las ideas que se quieren transmitir. Así, cuando alguien saluda, lo puede decir de varias formas (*Hola, Buenos días, Hey ...*). Podremos controlar los ejemplos de las intenciones (y crear las nuestras propias) en `data/nlu.yml`\n",
    "\n",
    "Para dar las respuestas, Rasa usa dos conceptos:\n",
    "\n",
    "- **Declaraciones (o utterances)**: Son aquellas respuestas que sólo precisan de una serie de textos (Por ejemplo, si pedimos que salude, podemos decir que siempre responda con un Muy buenas, ¿qué tal?). Estas respuestas se declaran en la sección responses de domain.yml.\n",
    "- **Acciones (o actions)**: Son aquellas respuestas más complejas que necesitan de información dinámica (como poner frases aleatorias o decir la hora). Estas respuestas se declaran en la sección actions de domain.yml, y se deben programar en actions/actions.py (se explicará cómo hacerlo en secciones posteriores).\n",
    "\n",
    "Para poder saber cómo relacionar las preguntas y las respuestas, tenemos dos maneras. Por una parte, podemos hacer que sigan reglas de forma que\n",
    "cada vez que se manifieste una intención, debemos siempre relacionarlo con una respuesta. Estas reglas se pueden establecer en el archivo data/rules.yml\n",
    "Por otra parte, si queremos que sigan un flujo de conversación, podemos hacer historias de usuario, donde representamos una serie de intenciones\n",
    "y sus respuestas en forma de acción o declaración, que podrán usarse para entrenar el modelo a base de ejemplos. Estas historias de usuario podemos tenerlas como parte del entrenamiento o como parte de la validación (que quedan aparte del entrenamiento para comprobar que lo aprendido se puede exportar a otros casos parecidos). Los casos de uso de entrenamiento se podrán introducir en data/stories.yml, y los de validación, en tests/test_stories.yml\n",
    "Cuando se hagan cambios en el modelo, hay que volverlo a entrenar. Desde el archivo de configuración (config.yml) podemos hacer ajustes en las políticas de entrenamiento.\n",
    "\n",
    "Para entrenar el modelo en Rasa invocaremos el siguiente comando; quien usando los casos de uso de entrenamiento como de validación, podrá aprender a entender las intenciones y dar así una respuesta.\n",
    "\n",
    "```bash\n",
    "rasa train\n",
    "```\n",
    "\n",
    "Para probarlo, invocaremos este otro comando.\n",
    "```bash\n",
    "rasa shell\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conectando el servidor de Rasa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner a Rasa en modo de servidor, podremos lanzar este comando:\n",
    "\n",
    "```bash\n",
    "rasa run\n",
    "```\n",
    "\n",
    "El puerto se puede configurar en `endpoints.yml`. \n",
    "\n",
    "Al configurar el puerto y lanzar el servidor podemos hacer peticiones a Rasa desde otros scripts, incluso otros PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "sender = input(\"What is your name?\\n\")\n",
    "\n",
    "bot_message = \"\"\n",
    "while bot_message != \"Adiós\":\n",
    "    message = input(\"What's your message?\\n\")\n",
    "\n",
    "    print(\"Sending message now...\")\n",
    "\n",
    "    r = requests.post('http://localhost:5005/webhooks/rest/webhook', json={\"sender\": sender, \"message\": message})\n",
    "\n",
    "    print(r.json())\n",
    "\n",
    "    print(\"Bot says, \")\n",
    "    for i in r.json():\n",
    "        try:\n",
    "            bot_message = i['text']\n",
    "        except KeyError:\n",
    "            try:\n",
    "                bot_message = i['image']\n",
    "            except KeyError:\n",
    "                bot_message = ''\n",
    "        print(f\"{bot_message}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Funciones custom"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones custom se pueden lanzar usando:\n",
    "\n",
    "```bash\n",
    "rasa run actions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el archivo chatbot/actions/actions.py ...\n",
    "\n",
    "from rasa_sdk import Action, Tracker\n",
    "from rasa_sdk.executor import CollectingDispatcher\n",
    "import requests\n",
    "\n",
    "class ActionTellTime(Action):\n",
    "    \"\"\"\n",
    "    Clase para la acción de decir la hora\n",
    "    \"\"\"\n",
    "\n",
    "    def name(self) -> Text:\n",
    "        \"\"\"\n",
    "        Declaración de la acción\n",
    "        @return string Nombre de la acción\n",
    "        \"\"\"\n",
    "        return \"action_tell_time\"\n",
    "\n",
    "    def run(self, dispatcher: CollectingDispatcher,\n",
    "            tracker: Tracker,\n",
    "            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n",
    "        \"\"\"\n",
    "        Ejecución de la acción.\n",
    "        Extrae el datetime de este instante y extrae la hora\n",
    "        @utter_message Devuelve la hora en formato HH y MM (15:30 -> Son las 15 horas y 30 minutos)\n",
    "        \"\"\"\n",
    "        dispatcher.utter_message(text=f\"Son las \\\n",
    "            {datetime.datetime.now().strftime('%H horas y %M minutos')}\")\n",
    "\n",
    "        return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Entidades\n",
    "\n",
    "Las entidades son palabras que en su conjunto podemos asociar a una temática. Por ejemplo, podemos asociar como países palabras como España, Italia, República Checa ...\n",
    "\n",
    "Nuestro chatbot no sabe asociar este tipo de palabras pero nosotros se lo podemos decir usando los ejemplos con `[ejemplo](tipo_entidad)`.\n",
    "\n",
    "Así, por ejemplo:\n",
    "\n",
    "```yaml\n",
    "- intent: city_weather\n",
    "  examples: |\n",
    "    - qué tiempo hace en [granada](city)\n",
    "    - cómo está el tiempo en [Málaga](city)\n",
    "    - cómo está el clima en [madrid](city)\n",
    "    - cómo está [Londres](city) hoy\n",
    "    - dime qué tiempo hace en [parís](city)\n",
    "    - dime el tiempo que hace en [Nueva York](city)\n",
    "    - que tiempo hace en [granada](city)\n",
    "    - dime el tiempo que hace en [sevilla](city)\n",
    "    - dime como esta el clima en [malaga](city)\n",
    "    - que tiempo hara en [barcelona](city)\n",
    "    - que tiempo hara en [sevilla](city)\n",
    "    - como esta el tiempo en [denia](city)\n",
    "    - que clima hace en [granada](city)\n",
    "    - que tiempo hara en [huelva](city)\n",
    "```\n",
    "\n",
    "Podemos usar las entidades capturadas en las acciones. Mira este ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En actions.py...    \n",
    "    \n",
    "    def run(self, dispatcher: CollectingDispatcher,\n",
    "            tracker: Tracker,\n",
    "            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n",
    "\n",
    "        city = next(tracker.get_latest_entity_values('city'),None)\n",
    "       \n",
    "        dispatcher.utter_message(text=f'Ahora mismo no te puedo decir mucho del tiempo\\\n",
    "                que hace en {city}. Consulta a tu meteorólogo.')\n",
    "\n",
    "        return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 La respuesta por defecto\n",
    "\n",
    "Hay ocasiones donde no sabemos qué responder porque puede ser algo estúpido o porque está fuera de tu dominio.\n",
    "\n",
    "Por ejemplo, alguien puede preguntar algo como :\n",
    "\n",
    "![SkullGirls Question](https://www.quartertothree.com/fp/wp-content/uploads/2014/07/riddles_in_the_skull.jpg)\n",
    "\n",
    "Ante esta cuestión, Rasa \"explota\" una mijilla y pone una respuesta por defecto, pero podemos cambiar su comportamiento.\n",
    "\n",
    "Para ello podemos crear una intención llamada `nlu_fallback`:\n",
    "\n",
    "```yaml\n",
    "- intent: nlu_fallback\n",
    "  examples: |\n",
    "    - amo a ver que dise\n",
    "    - mefovjsfocjsobjdsvo\n",
    "    - vkoskvpskvsspvpskvjisvhsihvsihvsiaa\n",
    "    - aclmacm\n",
    "    - asmadajcipadpdvaj\n",
    "    - vsdpdkvpsdkvpskpvks\n",
    "    - papapapapapapapappapapa\n",
    "    - mamamamamam\n",
    "    - adios\n",
    "```\n",
    "\n",
    "Con ello, podemos crear una regla o historia de usuario donde pongamos por intención el fallback y como respuesta lo que uno quiera."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Juntándolo todo\n",
    "\n",
    "Una vez vistos los snippets de código y cómo funcionan, podremos seguir un flujo como el siguiente para ordenar los snippets como funciones y crear nuestro asistente.\n",
    "\n",
    "![Diagrama Flujo](img/DiagramaIntuitivo.png)\n",
    "\n",
    "## 5.1 Algunas cuestiones\n",
    "\n",
    "- La voz de eSpeak es muy robótica.\n",
    "\n",
    "Era la única que permitía funcionar en todas las plataformas. Hay otros TTS geniales como NanoTTS en el caso de Linux.\n",
    "\n",
    "- ¿Y si lo queremos llevar a un sistema como una Raspi?\n",
    "\n",
    "En versiones con poca RAM como la 3B no se puede ejecutar Rasa lamentablemente (aunque se le puede hace funcionar igualmente ajustando el programa como un Cliente/Servidor). En la 4 de 4GB ya se ha reportado un buen funcionamiento (lento, pero funciona)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ba91e5e8cac5424b3e03084f20effd4266e6668cb94b70218f91abde236bd39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
